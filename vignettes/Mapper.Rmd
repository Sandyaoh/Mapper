---
author: "Matt Piekenbrock"
title: "Using the Mapper Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Mapper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document contains a quick overview of how to use the interface in the _Mapper_ package to fully customize the _mapper_ construction. The focus on this document is on package functionality and usage, as opposed the theory the _Mapper_ framework was founded on, and as such discusses _Mapper_ from a strictly algorithmic / computational perspective. 

```{r}
m <- MapperRef$new(X = noisy_circle)$
  use_filter(filter= f_x)$
  use_cover(cover = "fixed interval", number_intervals = 5, percent_overlap = 20)
```


# Quickstart 

```{r setup, echo=FALSE}
library("Mapper")
```

Consider a noisy sampling of points along the perimeter of a circle in $\mathbb{R}^2$
```{r}
set.seed(1234)

## Generate noisy points around the perimeter of a circle 
n <- 1500
t <- 2*pi*runif(n)
r <- runif(n, min = 2, max = 2.1)
noisy_circle <- cbind(r*cos(t), r*sin(t))

## Plot the circle
plot(noisy_circle, pch = 20, asp = 1, xlab = "X", ylab = "Y", main = "Circle")
```

To get the __mapper__ of this circle, first supply the data via `X` and the mapped values via `filter_values`, along with the parameters to other components such as the cover and clustering parameters. See `?mapper` for more details. A summary is available with the default print method.
```{r}
library("Mapper")
left_pt <- noisy_circle[which.min(noisy_circle[, 1]),]
f_x <- matrix(apply(noisy_circle, 1, function(pt) (pt - left_pt)[1]))
m <- mapper(X = noisy_circle, filter_values = f_x, 
            cover_params = list(typename="fixed interval", number_intervals=10L, percent_overlap=50),
            measure = "euclidean", 
            cluster_params = list(cl="single", threshold = 0.0))
print(m)
```
By default, the core information of the $1$-skeleton of the `Mapper` construction is returned, including:
  
  1. The vertices, and the points (by index) comprising their corresponding connected components.
  
  2. An adjacency representation of the mapper 
  
  3. A map between the index set of the cover and the vertices

This is, in essence, all the central information one needs to conduct a more in-depth analysis of the mapper.

# Building the Mapper piecewise 

For most use-cases, the static method above is sufficient for getting a compact Mapper construction back. However, building the 'right' Mapper is usually a _process_ that requires parameter tuning in the form of e.g. alternative filter functions, 'fitted' clustering parameters, different covering geometries, etc. For large data sets, employing this iterative fitting process with the above method can be prohibitively expensive.

To help facilitate this process, the _Mapper_ package uses `R6` class generators to decompose the components used to build the mapper. A few of the benefits `R6` classes include full encapsulation support, reference semantics for member fields, easy-to-employ parameter checking via active bindings, and composability via [method chaining](https://adv-r.hadley.nz/R6.html#method-chaining). More details available at [Hadleys Advanced R](https://adv-r.hadley.nz/r6.html#introduction-12) series or the [R6 documentation site](https://r6.r-lib.org/articles/Introduction.html).

To demonstrate some of these benefits, consider a simplified interpretation of the Mapper pipeline:

  1. "Filter" the data via a reference map
  
  2. Equip the filtered space with a cover
  
  3. Decompose preimages of the (pulled back) open sets of the cover into _clusters_
  
  4. Construct the _k_-skeleton by considering non-empty intersections of the clusters
  
These steps are demonstrated below using the `noisy_circle` data set below. To begin building a mapper object more fluidly, one starts by using the `MapperRef` R6-class instance generator. It accepts as its only argument the data to construct the mapper with. The data can be specified as a matrix of coordinate values or a function which returns a matrix of coordinate values. 

```{r}
m <- MapperRef$new(X = noisy_circle)
```

Assume, as before, that our filter is the distance from every points first coordinate to the point with left-most coordinate, $p$, i.e. 

```{r}
## 1. Specify a filter function for the data 
left_pt <- noisy_circle[which.min(noisy_circle[, 1]),]
f_x <- matrix(apply(noisy_circle, 1, function(pt) (pt - left_pt)[1]))
```

Coloring the points in the original space w/ a rainbow gradient based on their filter values sheds light on what the filter geometrically represents
```{r}
## Bin the data onto a sufficiently high-resolution rainbow gradient from blue (low) to red (high)
rbw_pal <- rev(rainbow(100, start = 0, end = 4/6))
binned_idx <- cut(f_x, breaks = 100, labels = F)
plot(noisy_circle, pch = 20, asp = 1, col = rbw_pal[binned_idx], xlab = "X", ylab = "Y", main = "Circle")
```

The filter is assigned as usual, and like the data matrix, it accepts a matrix of coordinate values or a function that generates a matrix of coordinate values.
```{r}
m$filter <- f_x
```

Both the data matrix and filter as stored as _functions_, which both accept as input a set of indices and return as output a matrix of coordinate values.

<!-- Explain sqllite example -->

## Constructing the cover

Once the filter has been applied, a cover must be chosen to discretize the space. Here, a simple rectangular cover with fixed centers is used. 
```{r}
## 2. Create cover 
cover <- FixedIntervalCover$new(number_intervals = 5L, percent_overlap = 20)
```
Cover parameters may be be supplied at initialization, or set via the assignment operator. 
If you supply a single value when the filter dimensionality $> 1$, the argument is recycled across the dimensions. The cover summary can be printed as follows: 

```{r}
print(cover)
```

Once parameterized, the cover may be explicitly constructed via the `construct_cover` member, before sending to Mapper. The `construct_cover` function uses the given set of parameters to populate the intersection between the open sets in the cover and the given filter data.

```{r}
cover$construct_cover(filter = m$filter) ## Now the level sets are populated
summary(cover$level_sets)
```
```{r}
m$cover <- cover
```

`Mapper` accepts any cover that derives from the abstract `CoverRef` R6 class. To see a list of the `covers_available` method. 
```{r}
## Print the typename/identifer, the R6 generator, and the parameter names
Mapper::covers_available()
```

If you want to use a cover outside of the ones offers by the package, feel free to derive a new type cover class (and submitting a pull request!).

## Decomposing the preimages

After creating the cover, the next step is to perform the _pullback_ operation, wherein the cover $\mathcal{U}$ of the space $Z$ is used to create a cover on $X$ through $f$. This operation generally denotes two operations: computing the preimages of each open set, and decomposing the subsets of $X$ given by these preimages into connected components. The latter is referred to in the original Mapper paper as _partial clustering_, and it (naturally) requires a number of parameters. 

The goal of the _Mapper_ package is to provide the user full access to how the open sets are decomposed, while also providing easy-to-parameterize defaults for those unfamiliar with clustering in general. Towards this end, there are two options to assign the clustering algorithm: either choose from a set of pre-defined 
clustering algorithms (+ hyper-parameters), or simply supply your own function to do the decomposition.

#### Using pre-configured defaults

For people do not have much experience in clustering, one may simply one of many pre-defined [linkage criteria](https://en.wikipedia.org/wiki/Hierarchical_clustering) to the `cl` argument of the `use_clustering_algorithm` method. The choice of linkage criteria determines the type of hierarchical clustering to perform. Given a cluster hierarchy, it's common to `cut` the tree at some height value to obtain a partitioning of the points. There are a few available cutting heuristics implemented which attempt to identify the major 'clusters' automatically. See `?use_clustering_algorithm` for more details.

Below is an example of how to specify a pre-configured clustering approach.

```{r}
m$use_clustering_algorithm(cl = "single", cutoff_method = "continuous")
```

For a complete list of the available linkage criteria, see `?stats::hclust`.

#### Using a custom function

If it is not sufficient to use one of the standard clustering methods, you can alteratively just supply a  function! The only requirement is that it take input the index of the open set `pid` and a vector of indices `idx`$\subset \{1, 2, \dots, n\}$ (representing the preimage), and return an integer vector giving a partitioning on that subset. An example of how one might construct a custom clustering function leveraging the `parallelDist` and `fastcluster` packages is given below.

```{r}
custom_clustering_f <- function(num_bins.default = 10L){
  dependencies_met <- sapply(c("parallelDist", "fastcluster"), requireNamespace)
  stopifnot(all(dependencies_met))
  function(pid, idx) {
    dist_x <- parallelDist::parallelDist(self$X(idx), method = self$measure)
    hcl <- fastcluster::hclust(dist_x, method = "single")
    eps <- cutoff_first_bin(hcl, num_bins = num_bins.default)
    cutree(hcl, h = eps)
  }
}
m$use_clustering_algorithm(cl = custom_clustering_f(10L))
```

The above example also demonstrates a simplified example of what one of the pre-configured options might look like. Functions supplied via the `use_clustering_algorithm` accessor are given access to the instance fields and methods, e.g. the data `self$X(...)` and the measure with `self$measure`. 

By default, the metric to cluster on is set to euclidean distance. However, the distance measure may also be any one of the 49 (at this time of writing) proximity measures available in from the `proxy` package. To change the distance measure, use the `use_distance_measure` method. 
```{r}
m$use_distance_measure(measure = "euclidean")
```
For a complete list of the distance/similarity measures supported, see `?proxy::pr_DB`.

Notice the clustering function in this example was created as a [closure](https://en.wikipedia.org/wiki/Closure_(computer_programming)). Since closures [capture their enclosing environments](http://adv-r.had.co.nz/Functional-programming.html#functional-programming), they can be quite useful for e.g. storing auxiliary parameters option needed by the algorithm, checking for package dependencies (like above), caching precomputed information to for improved efficiency, etc. However, for the same reason, they are a common cause of a form of [memory leaking](http://adv-r.had.co.nz/memory.html#gc), so they should generally be used with caution.

#### Performing the decomposition

Once the clustering algorithm has been picked, the pullback can be constructed.
```{r}
m$construct_pullback()
```

This populates the vertices the _mapper_ construct and records a mapping between the index set of the cover and the corresponding vertices decomposed by the corresponding open set. 

```{r}
str(m$vertices)
str(m$pullback)
```


## Constructing the k-skeleton 

The final step to create a Mapper is to construct a geometric realization of nerve of the pullback: a simplicial complex. Internally, the simplicial complex is stored in a [Simplex Tree](https://hal.inria.fr/hal-00707901v1/document). The underlying simplex tree is exported as an [Rcpp Module](https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-modules.pdf), and is accessible via the `$simplicial_complex` member. 
```{r}
m$simplicial_complex
```

The simplex tree is a type of trie specialized specifically for storing and running general algorithms on simplicial complexes. For more details on the simplex tree, see use `?simplextree`, or see the [package repository](https://github.com/peekxc/simplextree/), and references therein. 

Often, instead of constructing the full-dimensional complex (where the dimension is governed by the maximum number of non-empty intersections in the cover), it's common to restrict the _mapper_ to the _k_-skeleton of the nerve instead. 

For example, if ones wishes to reduce the _mapper_ to a graph, the $1$-skeleton may be computed as follows.
```{r}
m$construct_k_skeleton(k = 1L)
m$simplicial_complex
```

Generally, this is sufficient. However, if necessary, the $k$-skeleton for any $k > 1$ can also be computed. 
```{r}
m$construct_k_skeleton(k = 2L)
m$simplicial_complex
```

Note that `construct_k_skeleton` clears out the simplicial complex, the decomposed preimages, etc. and iteratively builds the nerve (up to the specified dimension) each time it's called. Alternatively, each dimension of the nerve can also be computed separately, so long as the previous dimension has been already been computed, i.e. 

```{r}
m$simplicial_complex$clear()
m$construct_nerve(k = 0L)
m$construct_nerve(k = 1L)
# ... 
```

Using the `construct_nerve` method is only necessary if one wants to obtain finer control over the specific intersection checks. Just the the `construct_pullback` method, it optionally accepts a subset of indices which can be used to restrict the intersection computation to specific subsets of the data.  


## Putting it all together 

The above is a brief exposition into how to customize each component of the Mapper process. To facilitate easier parameterization and composability, by convention all the methods denoted with the `use_*` enact side-effects and return the instance invisibly. This enables a reduction of the Mapper `pipeline` to a sequence of chained operations:
```{r}
m <- MapperRef$new(noisy_circle)$
  use_filter(filter = f_x)$
  use_cover(cover = "fixed interval", number_intervals = 5L, percent_overlap = 20)$
  use_distance_measure(measure = "euclidean")$
  use_clustering_algorithm(cl = "single")$
  construct_k_skeleton(k=1L)
```

If the Mapper is constantly being reparameterized, this interface is preferred over the more concise `mapper` call because of the efficiency that comes with mutability. To illustrate this, consider the case where you want to use a different distance measure for the clustering algorithm. To change the metric with the 'static' approach, one might do something like the following:
```{r}
all_params <- list(
  X = noisy_circle, filter_values = f_x, 
  cover_params = list(typename="restrained rectangular", 
                      number_intervals=10L, percent_overlap=50), 
  cluster_params = list(cl="single", num_bins=10L)
)

m_static1 <- do.call(mapper, append(all_params, list( measure = "euclidean")))
m_static2 <- do.call(mapper, append(all_params, list( measure = "mahalanobis")))
```

Both function calls require the entire mapper to be reconstructed, including the cover, the allocation for the simplex tree, all initializers, etc. With reference semantics, you can keep all the same settings and just change what is needed. For example, the following switches the distance measure and exports `mapper` objects without reconstructing the cover:
```{r}
m_dynamic <- lapply(c("euclidean", "mahalanobis"), function(measure){
  m$use_distance_measure(measure)
  m$compute_k_skeleton(k=1L)$exportMapper()
})
```

Note that, due to the a natural dependency chain that comes with Mapper, the pullback and nerve operations both needed to be recomputed, since the latter depends on the former, and the former (the pullback operation) depends on the distance measure. Determining what requires recomputation depends on how far up the dependency chain you go. For example, consider recomputing the 1-simplices with different intersection size thresholds.
```{r}
invisible(lapply(1:10, function(e_weight){
  m$construct_nerve(k = 1L, min_weight = e_weight)
}))
```
In this case, compared to the static method, we save on having to recompute the cover, the clustering, and the construction of the pullback.

## Visualizing the Mapper 

The `pixiplex` package provides a [htmlwidget](https://www.htmlwidgets.org/) implementation of a subset of the [pixi.js](https://www.pixijs.com/) geared for visualizing and interacting with simplicial complexes, which can be used to visualize the _mapper_ construction. An export option is available via the `$as_pixiplex` function, which provides a few default coloring and sizing options.
```{r, eval=TRUE, echo=TRUE}
library("pixiplex")
plot(m$as_pixiplex())
```

For more information, see the [package repository](https://github.com/peekxc/pixiplex).

For a static rendering of the _mapper_, one can use either the `as_igraph` method or, for higher-order complexes, the default S3 plot method available in the simplextree package (e.g. `plot(m$simplicial_complex)`). Other options for visualization are an ongoing effort. If you have a particular visualization package you prefer, feel free to [submit a issue or PR on the github repository](https://github.com/peekxc/Mapper).


<!-- If the clustering function depends on internal components of the `MapperRef` class instance, the function will error when it is first called. Consider the exact same code as above, but this time parameterized by a field of the `MapperRef` instance. -->

<!-- ```{r, eval = FALSE, echo = TRUE} -->
<!-- m$measure <- "euclidean" -->
<!-- custom_clustering_f <- function(){  -->
<!--   ... -->
<!--   dist_x <- parallelDist::parallelDist(X[idx,], method = self$measure) -->
<!--   ... -->
<!-- } -->
<!-- ``` -->


<!-- This will return an evaluation error upon executing, both from the user side and internally.  -->
<!-- ```{r, eval = FALSE} -->
<!-- m$clustering_algorithm(idx = 1:10) -->
<!-- ## Error in pmatch(method, METHODS) : object 'self' not found  -->
<!-- ``` -->

<!-- The search path of the execution environment of the closure doesn't traverse the binding environment of either the _self_ or _private_ environments associated with the instance. If access to these environments is needed, pass the clustering function via the `cl` argument of the `use_clustering_algorithm` member.  -->

<!-- ```{r} -->
<!-- m$use_clustering_algorithm(cl = custom_clustering_f(), run_internal = TRUE) -->
<!-- ``` -->

<!-- This effectively allows the user to use any clustering algorithm they wish, even algorithms that depend on the internal parameters of the _mapper_ itself.  -->

<!-- The only requirements of the function is that it accept as its arguments the index of the open set to peform the partial clustering and the indices of the points in the preimage of that open set, and return as its output a vector of integers indicating component membership. -->